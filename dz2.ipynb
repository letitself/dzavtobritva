{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd77db22",
   "metadata": {},
   "source": [
    "Импортируем нужные модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d220f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e792e4",
   "metadata": {},
   "source": [
    "Взял текст с сайта зе флоу, объясняю почему он: много сложных случаев таких как имена артистов всякие жаргонизмы и прочее. В тексте фигурирует множество дат и указаний на время, а еще тут много английских слов. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ff1a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "В огромном интервью Олег Савченко вспоминает всё: как его карьера чуть было не закончилась, даже не успев начаться; как он писал песни про магию в тамбурах поездов и комнатах студенческих общежитий, как состоял в одном рэп-объединении с Максом Коржом, и как потерял одну из песен, написанных для этого релиза (но он нам ее напел; как говорят в таких случаях: worldwide exclusive!).\n",
    "\n",
    "Олег Савченко вспоминает, как вдохновлялся \"хипстер-хопом\" (кто сегодня вспомнит, что это вообще такое?), искал свою формулу и \"начал подозревать, что читать необязательно\"; как написал альбом и просто отправил на 3 почты — и вдруг о нем заговорили Афиша и Rap.ru, а снятый за копейки клип показали на ТВ. Что было дальше, вы знаете сами.\n",
    "\n",
    "В 2007-м я поступил в универ и переехал из Витебска в Минск. Я уже знал про минскую тусу M.U Skool, где собирались самые крутые рэперы Беларуси. Поначалу это была тусовка из 10 человек, среди них — локальные рэп-герои Deech, Вожык, Zeman, Reezah Realize и Сашок, чувак, который всю эту херню двигал и спонсировал. Из кружка по интересам это превратилось в тусы, куда в пиковый период приходило по 200 человек.\n",
    "\n",
    "Местом сбора была станция метро Спортивная. По субботам в 19:00 все туда съезжались, закупались пивасом, чипсами, рыбой, и шли в дальний переход. Обязательно был кто-то с бумбоксом, ставили только биты конца 90-х. Все, что с рэпом происходило позже, старались не замечать. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104203e9",
   "metadata": {},
   "source": [
    "Сначала токенизиру текст в цсв файл а затем руками размечаю каждый токен при помози экселя затем импортирую обратно из формата хслс в цсв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92c12592",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = WordPunctTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00bf9685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, ['В', 'огромном', 'интервью', 'Олег', 'Савченко'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens),tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f213b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for w in tokens:\n",
    "        writer.writerow([w, ' '])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bbb91c",
   "metadata": {},
   "source": [
    "по итогу у нас получается файл с золотым стандартом with_tags.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "16792c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = []\n",
    "with open('with_tags.csv', 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            tagged.append((row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "66cb732c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(264,\n",
       " [('В', 'ADP'),\n",
       "  ('огромном', 'ADJ'),\n",
       "  ('интервью', 'NOUN'),\n",
       "  ('Олег', 'PROPN'),\n",
       "  ('Савченко', 'PROPN')])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged), tagged[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2d8e1",
   "metadata": {},
   "source": [
    "Теперь для того чтобы сравнить золотой стандарт с другими морфоанализаторами, и понять насколько они точные, надо выбрать эти самые морфоанализаторы и распарситьт с помощью них текст. В своей работе я возьму пайморфи станзу и наташу"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46630091",
   "metadata": {},
   "source": [
    "Начну с наташи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee753f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natasha\n",
      "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.4 MB 126 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pymorphy2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from natasha) (0.9.1)\n",
      "Collecting navec>=0.9.0\n",
      "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
      "Collecting slovnet>=0.3.0\n",
      "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 178 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yargy>=0.14.0\n",
      "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 88 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Collecting razdel>=0.5.0\n",
      "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Collecting intervaltree>=3\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "Collecting sortedcontainers<3.0,>=2.0\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from navec>=0.9.0->natasha) (1.21.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Using legacy 'setup.py install' for intervaltree, since package 'wheel' is not installed.\n",
      "Installing collected packages: sortedcontainers, razdel, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
      "    Running setup.py install for intervaltree ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 razdel-0.5.0 slovnet-0.5.0 sortedcontainers-2.4.0 yargy-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d35198ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "segmenter = Segmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98f01b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_n = Doc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2524a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_n.segment(segmenter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc5da1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_n.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a69b2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_natasha = [(_.text, _.pos) for _ in text_n.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9af4d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('В', 'ADP'),\n",
       " ('огромном', 'ADJ'),\n",
       " ('интервью', 'NOUN'),\n",
       " ('Олег', 'PROPN'),\n",
       " ('Савченко', 'PROPN')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_with_natasha[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "10da2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('natasha.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for c in text_with_natasha:\n",
    "        writer.writerow([c[0], c[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2deb737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntsh = []\n",
    "with open('natasha.csv', 'r', encoding = 'UTF-8') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            ntsh.append((row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3d98de3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('В', 'ADP'),\n",
       " ('огромном', 'ADJ'),\n",
       " ('интервью', 'NOUN'),\n",
       " ('Олег', 'PROPN'),\n",
       " ('Савченко', 'PROPN')]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntsh[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3e9d6",
   "metadata": {},
   "source": [
    "Станза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "987a6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.2.3-py3-none-any.whl (342 kB)\n",
      "\u001b[K     |████████████████████████████████| 342 kB 76 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting torch>=1.3.0\n",
      "  Downloading torch-1.9.1-cp39-none-macosx_10_9_x86_64.whl (218.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 218.8 MB 2.2 MB/s eta 0:00:012   |█▋                              | 11.2 MB 289 kB/s eta 0:11:58     |██████▉                         | 46.6 MB 1.9 MB/s eta 0:01:31\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from stanza) (4.62.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from stanza) (2.26.0)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-3.18.0-cp39-cp39-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from stanza) (1.21.2)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->stanza) (2.0.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->stanza) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->stanza) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from requests->stanza) (3.2)\n",
      "Installing collected packages: typing-extensions, torch, protobuf, stanza\n",
      "Successfully installed protobuf-3.18.0 stanza-1.2.3 torch-1.9.1 typing-extensions-3.10.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aaec3d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef1f95a100545f3b92e0ef9bfc52302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 14:41:29 INFO: Downloading default packages for language: ru (Russian)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fbc4e610454ac1b365f74e5fce2932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading http://nlp.stanford.edu/software/stanza/1.2.2/ru/default.zip:   0%|          | 0.00/574M [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 14:51:26 INFO: Finished downloading models and saved to /Users/tarasandrushko/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e411858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-02 14:55:24 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| pos       | syntagrus |\n",
      "=========================\n",
      "\n",
      "2021-10-02 14:55:24 INFO: Use device: cpu\n",
      "2021-10-02 14:55:24 INFO: Loading: tokenize\n",
      "2021-10-02 14:55:24 INFO: Loading: pos\n",
      "2021-10-02 14:55:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='RU', processors='tokenize,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "faffe9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "29b2721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_stanza = []\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        text_with_stanza.append((word.text, word.upos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "125ed5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('В', 'ADP'),\n",
       " ('огромном', 'ADJ'),\n",
       " ('интервью', 'NOUN'),\n",
       " ('Олег', 'PROPN'),\n",
       " ('Савченко', 'PROPN')]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_with_stanza[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "134911f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stanza.csv', 'w', newline='', encoding = 'UTF-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for s in text_with_stanza:\n",
    "        writer.writerow([s[0], s[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c39ebe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "stnz = []\n",
    "with open('stanza.csv', 'r', encoding = 'UTF-8') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            stnz.append((row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2e5597c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('В', 'ADP'),\n",
       " ('огромном', 'ADJ'),\n",
       " ('интервью', 'NOUN'),\n",
       " ('Олег', 'PROPN'),\n",
       " ('Савченко', 'PROPN')]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stnz[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023f6bf",
   "metadata": {},
   "source": [
    "Pymorphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c01e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f2008914",
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_lib = []\n",
    "with open('with_tags.csv', 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=';')\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            pymorphy_lib.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "792b4e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pymorphy_with_tags = []\n",
    "for lib in pymorphy_lib:\n",
    "    parse = morph.parse(lib)[0]\n",
    "    pos = parse.tag.POS\n",
    "    pymorphy_with_tags.append((lib, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a0ac5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('В', 'PREP'),\n",
       " ('огромном', 'ADJF'),\n",
       " ('интервью', 'NOUN'),\n",
       " ('Олег', 'NOUN'),\n",
       " ('Савченко', 'NOUN')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy_with_tags[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "129d5253",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pymorphy.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for column in text_with_stanza:\n",
    "        writer.writerow([column[0], column[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9ea91",
   "metadata": {},
   "source": [
    "Сравним точность парсеров с золотым стандартом при помощи функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "35fa8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "tagged = []\n",
    "with open('with_tags.csv', 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            tagged.append((row[0], row[1]))\n",
    "            \n",
    "def get_accuracy(file):\n",
    "    with open(file, 'r') as csv_file:\n",
    "        corp = []\n",
    "        reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in reader:\n",
    "            if len(row) == 2:\n",
    "                corp.append((row[0], row[1]))\n",
    "    \n",
    "    for tc in range(len(tagged)):\n",
    "        check = 0\n",
    "        if tagged[tc][0] == corp[tc][0]:\n",
    "            if tagged[tc][1] == corp[tc][1]:\n",
    "                accuracies.append(1)\n",
    "                check = 1\n",
    "            else:\n",
    "                accuracies.append(0)\n",
    "                check = 1\n",
    "        else:\n",
    "            for c in corp:\n",
    "                if c[0] == tagged[tc][0]:\n",
    "                    if c[1] == tagged[tc][1]:\n",
    "                        accuracies.append(1)\n",
    "                        check = 1\n",
    "                        break\n",
    "        if check == 0:\n",
    "            accuracies.append(0)\n",
    "    return accuracies\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7705b7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4053030303030303\n"
     ]
    }
   ],
   "source": [
    "print(sum(get_accuracy('stanza.csv'))/len(get_accuracy('stanza.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c9d9df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5975378787878788\n"
     ]
    }
   ],
   "source": [
    "print(sum(get_accuracy('natasha.csv'))/len(get_accuracy('natasha.csv')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6395ac9d",
   "metadata": {},
   "source": [
    "Не очень точно от станзы, если честно даже обидно что пришлось для этого скачать гиг....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aed3f5",
   "metadata": {},
   "source": [
    "Теперь чанкиЪ сделаю для наташи тк она точнее оказалась"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7f232f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunckies(corpus):\n",
    "    noun = []\n",
    "    verb = []\n",
    "    vnoun = []\n",
    "    for c in range(len(corpus) - 1):\n",
    "        if corpus[c][1] == 'ADJ' and corpus[c+1][1] == 'NOUN':\n",
    "            noun.append((corpus[c][0], corpus[c+1][0]))\n",
    "        if corpus[c][1] == 'NOUN' and corpus[c+1][1] == 'VERB':\n",
    "            verb.append((corpus[c][0], corpus[c+1][0]))\n",
    "        if corpus[c][0].lower() == 'в' and corpus[c+1][1] == 'NOUN':\n",
    "            vnoun.append((corpus[c][0], corpus[c+1][0]))\n",
    "    return noun, verb, vnoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b2de16e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha = []\n",
    "with open('natasha.csv', 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in reader:\n",
    "        if len(row) == 2:\n",
    "            natasha.append((row[0], row[1]))\n",
    "noun, verb, vnoun = chunckies(natasha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b037f370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('огромном', 'интервью'),\n",
       " ('студенческих', 'общежитий'),\n",
       " ('минскую', 'тусу'),\n",
       " ('крутые', 'рэперы'),\n",
       " ('локальные', 'рэп-герои'),\n",
       " ('пиковый', 'период'),\n",
       " ('дальний', 'переход')]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "fe241294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('клип', 'показали'), ('период', 'приходило'), ('рэпом', 'происходило')]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "59368367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 'тамбурах'), ('в', 'универ'), ('в', 'тусы')]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vnoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b788049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
